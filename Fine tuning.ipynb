{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hZNSDOBJcBm"
   },
   "source": [
    "# Assignment III: Fine-Tuning of Language Models with QLoRA\n",
    "\n",
    "In this third assignment we will continue to work with PyTorch and Open AI's early Open Source Model GPT2 to develop a deeper understanding and intuition of how language models are fine-tuned with parameter efficient fine tuning (PEFT) techniques. We will continue to look at a specific simple task, Sentiment Classification, and see how we can fine tune two different models to improve performance.\n",
    "\n",
    "The structure of the Assignment is as follows:\n",
    "\n",
    "1. **Fine-tuning of [GPT2 large](https://huggingface.co/openai-community/gpt2-large) with a sentiment classification dataset**  \n",
    "\n",
    "   Here we will explore how we can combine Low Rank Adaptation (LoRA) with Quantization to fine tune a larger model.  We'll leverage the libaries from [Hugging Face](https://huggingface.co/docs/transformers/index) to use their AutoModel and AutoTokenizer classes as well as their Trainer class to fine tune a model to do sentiment classification. We'll experiment with some of the hyperparameters that affect LoRA performance to see what makes a positive or negative contribution.\n",
    "   We will learn that the Huggingface infrastructure allows us to easily fine tune much larger models than we could normally fit on our compute resources.\n",
    "\n",
    "2. **Fine-tuning of Gemma model for Sentiment Analysis**  \n",
    "   We will then use a larger more recent model -- [Gemma 2](https://huggingface.co/google/gemma-2b) from Google -- to illustrate the benefits of an increase in the number of parameters and how it affects the performance of the model.  This model doubles the number or parameters but has also undergone a better pre-training regime and we would expect that to be reflected in the performance of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0IwczMZFtrF"
   },
   "source": [
    "For reference, please consider the Lecture material for weeks 2 - 5 as well as the two Special Session notebooks:\n",
    "\n",
    "* Intro to PyTorch I (Basics)\n",
    "* Intro to PyTorch II (Huggingface & Language Models)\n",
    "* All lesson material and notebooks for Unit 5\n",
    "\n",
    "\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "\n",
    "* This notebook needs to be run using a GPU. If you use Google Colab, a T4 chip is the recommendation.\n",
    "  \n",
    "* Questions are always indicated as **QUESTION:**, so you can search for this string to make sure you answered all of the questions. You are expected to fill out, run, and submit this notebook, as well as to answer the questions in the answers cells as you did in a1. Please do not remove the output from your notebooks when you submit them as we'll look at the output as well as your code for grading purposes.\n",
    "\n",
    "* \\### YOUR CODE HERE indicates that you are supposed to write code. All the way up to \\### END YOUR CODE     \n",
    "\n",
    "* **Important!!:** When you are done please re-run your notebook from beginning to end to that all of the seeds apply! This is very important!\n",
    "\n",
    "**AUTOGRADER:**\n",
    "\n",
    "- In each code block, do NOT delete the ### comment at the top of a cell (it's needed for the Gradescope grading!)\n",
    "  - No autograder tests and results on this assignment.\n",
    "  - You will get the full 42 points from the human graders for this assignment.\n",
    "  - You may upload as many times as needed in your time window to get full points\n",
    "  - The assignment needs to be named Assignment_3.ipynb to be graded from the autograder!\n",
    "  - The examples given are samples of how we will test/grade your code.\n",
    "    - Please ensure your code outputs the exact same information / format!\n",
    "    - In addition to the given example, the autograder will test other examples\n",
    "- Please format your input and output strings to be user friendly\n",
    "- Adding comments in your code is strongly suggested but won't be graded.\n",
    "- If you are stuck on a problem or do not understand a question - please come to office hours or ask questions (please don't post your code though). If it is a coding problem send a private email to your instructor.\n",
    "- We also have a number of TA tutors for extra help and 1 on 1 sessions!\n",
    "- You may use any libraries from the Python Standard Library for this assignment: https://docs.python.org/3/library/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peRi8BBqFtrF"
   },
   "source": [
    "## 0. Environment Setup\n",
    "\n",
    "Let us first install a few required packages. (You may want to comment this out in case you use a local environment that already has the suitable packages installed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOsHUjgdIrIW"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install datasets==2.21.0\n",
    "!pip install transformers\n",
    "!pip install accelerate -U            # Quantization, Distribution\n",
    "!pip install -q peft                  # LoRA\n",
    "!pip install -q evaluate\n",
    "!pip install bitsandbytes             # QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 38
    },
    "id": "UNcBwzfVsKyV",
    "outputId": "14b6e8dd-d05a-435b-de4a-a1b3e6d33787"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from peft import LoraConfig, TaskType, PeftModel, get_peft_model\n",
    "from peft import load_peft_weights, set_peft_model_state_dict\n",
    "from peft import PromptEncoderConfig, prepare_model_for_kbit_training\n",
    "\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import wandb\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1KcjhDHtwY8"
   },
   "source": [
    "Some useful definitions and functions we'll use (see Text Classification notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OlSoLxdtu6j"
   },
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def show_currently_allocated_gpu_mem():\n",
    "  torch.cuda.empty_cache()\n",
    "  mem = torch.cuda.memory_allocated()\n",
    "  print(f\"Current GPU memory allocation (GB): {mem/1024**3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "## 1. Data Setup\n",
    "\n",
    "We use the [GLUE dataset](https://gluebenchmark.com/), one of the original NLP \"benchmarks\", loading the data for the Stanford Sentiment Treebank task. We will also define the tokenizer for our first model (they all use the GPT2 tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "task = actual_task = \"sst2\"\n",
    "\n",
    "base_model_name = \"gpt2-large\"  # GPT2\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "80378070e84c437aa5b28d2f044d120e",
      "3eec4366f1754e93a8b2aab56a23cac0",
      "dad6dd92e57746dc9e7da10678d1f54e",
      "1ba6777c7af34e3da5ecb7f0e1eee8a5",
      "c14eeff1135d4ec18d0f710e1189e86d",
      "5a2f7d4d885b42c5b8af13eeb093b5a0",
      "acbdfa9a902c44999758b950051fc4e3",
      "446fab0cd75f4b029208fffd0b9962ae",
      "53bb03f2ed184f11923652a68cf326bc",
      "22b85b80b75049cc9efc753112c98c29",
      "80eb2c22afeb4585aa951df7bc6c5d8f",
      "d32b325717754aa4bac9d8231169c0bb",
      "9d80b46c5a9c47b3b4a9fde4d4f30a02",
      "041b2d9f34bb4fdcb9d41215724eeb9f",
      "4a7704037a9042468cfbca04295e2be1",
      "214d9b75f996426bb41156a0f6bfc983",
      "6f20b5dd7ebc4727a60c304a6846821c",
      "3647013568014d11be2641cef389dbd0",
      "1a86bce8e9f04cfeb4f494f5e23722ed",
      "5f12a097e80b45838c3065a1f3146c69",
      "d91ba3bcd7c641049642c470dc5dc8fc",
      "6611f027a90b49de9d1eddcdfab852f3",
      "9edf2eac7cdb4728b5761b1b57c73acb",
      "969734e56f154771bb2ac49bc94ff317",
      "cb5471790aed4f50b8d6ee7b711620b2",
      "aebd18ef2bc34104b3224c47b384321f",
      "e6e9a4ddf5a943f8af8dd3b57ddf169d",
      "1c812543f11547e3b79800dc56d84cc7",
      "54b347c3313c4118806bf1c69d4a117f",
      "550c1f8cfcf646baad6f790b269539e5",
      "f4fb7d37b98a4a479b098444ab562741",
      "c972f5ea005543a6b095a7497426363f",
      "9af1d8bba4714948b219a5309196deb2",
      "dd917373c6dd4f2f93aee1498175ca9e",
      "770786db512142fdae3231e61a55cb02",
      "360bc42617a749908bdf106fb7013a3e",
      "9f7412f61f374b49bf611b8d4d99ef74",
      "56d38cc13c43452199d7a9961d15daf9",
      "431f4e3c9bcf4977af5c58e1c9c2d5cf",
      "85ab54a1ab064afa9b4d0ec853b31b13",
      "a55479dca4e14f0b9f6d8c3d78e705fa",
      "12b934406ebc40c0a1be1541aa882725",
      "0c0e29e066f7408cb307b94bb14f06f4",
      "8ca66262e3274844a6c4ac12d26a0e40",
      "b2b21a3fc77a448d9c58473b93fef674",
      "214466d8bd5b4ebfa83f1ce3f7b93bff",
      "676f87f64b37493fb26f2dc918821fad",
      "87d8fe70d7b94af0a9564ef57f656efd",
      "a46240514d7e46878b39c27a9b533ea0",
      "3c2f7461b45547e1a64e890d559958c5",
      "30f9fa6d1ba54739ad46121938c8afda",
      "76f646a4ddcf43709afb74680293c639",
      "944ef5480608491ba534fa0c6b3400eb",
      "33d188a919ca494da34462a8a6a9937b",
      "ed6bb1fa44b042cfa307bb8512e5cb3c"
     ]
    },
    "id": "kvyeJEq7xmFr",
    "outputId": "5d7c17ff-190d-4a30-9617-c63427841b3e"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329,
     "referenced_widgets": [
      "4a5ef86319404a589429254d4ddd4ee2",
      "4e0a65fe590543b8bf61b275d03fe1c8",
      "eef2145e59364c26a38cc9c4669a842c",
      "762add37e4194e688fe78eeb0fe084b2",
      "d1a3402d3f664287b01e8d756e01bfc4",
      "686032aaac4b41138b85c478a5c82562",
      "ca38e07da1e7453e95048d69999b8397",
      "88a1dea12b5c4a4ebd33b305e6c1bc6e",
      "d285608059c64bc7878c4d67e9242c58",
      "b108c5dd8ad84c72a11c9dee05ed149d",
      "8437dc303b43472fab1e3bd66e71859a",
      "088922956f0342f895271acfc16442fa",
      "fb54c4b965484aa7a4f5a274022f20a9",
      "e21cfe0513e549a0bff9c129e89abd1b",
      "226b7f6557324cc6929ee06322a8e759",
      "5fc5e41944ed4cffa3a28a929a1cc8a9",
      "732c8c19f8d646faa5e6600ec03cb4ad",
      "54ae1e220e1940fc8d0f7e01bf4e87a4",
      "b82f879ec3ff4a7d9a73b0ae5cc350a4",
      "861a9deb0d654f6da351e79c0faeb57e",
      "e6ce0586d9df463aa150ef88cc7ba971",
      "737bc1186d51432a9a8dc37ffb587eb2",
      "482004cb853043c2ae02c4a729d1ef15",
      "b778c23306d4485196a761c5273449ac",
      "c5cd2a43746e4a318bb34ada06327e01",
      "69c1d604eb6e4595afc897121349648e",
      "2ad10e35015240958db45e9ad9b02c50",
      "36169d12b7a0454eb7903e1941c5f4f6",
      "471f3d2f70054eb6ad9a42e499b9bfa0",
      "652cd94986fc4b16a6375b52d78fc65c",
      "1132e5b075534aada60ef691207b0f52",
      "32b44903389f419991478b60b3c9c409",
      "b8d3e4fb03494a91a6ffa4dac3e6868d",
      "26b235fac1934be09180cf885b5a770d",
      "390984ce6b5f48e09eb18b52a2ce5f38",
      "4118715ac7ee448e82cbe9d2dcf46c76",
      "a7a3ecc910144e9bb3f2ed8af42fa0f2",
      "bc1309695f664c678671b4b5d5c65096",
      "17314d23d6114b72bb842532335fcb1b",
      "16230c0209fb4569b1b1c511b3d219aa",
      "4dda920e223b4e6aa6a04ccd239a4681",
      "d054c3b40ff246b48bd13e20d301ba7f",
      "0cb44967286c490eb0e8f7b3593b1ea6",
      "3074b9eafc0144519262b8ffeff57f8d",
      "5e2800bfca154cec9a24f70d91c206ef",
      "6adc0a95aefa473b8988bcb363359fc3",
      "ad3f9022d24d403982bf00edeb14b00f",
      "df6935cbee2248c9a2862e9dda5f11d3",
      "b75a02bd77414bdcb9231887aefe856d",
      "40bc39c6bbcb4392b4b9cb08545edce7",
      "6769b516f29247289c97eaa57bb4aeba",
      "615195860ec84df9ae49188230b0ab26",
      "7c79eb64d1284d35b796e6606d648385",
      "4e943a6e3a9543479c5e8932cf5332ae",
      "f02dc8ad21df47bd8deead8a5e7af674",
      "cc7f0168af52492a8938113ebaa46c1d",
      "541040854cdf40d498983d0729086a0e",
      "a34bdf21e9ae4625bc9f889fac98248e",
      "c703ecdd60b042959c398a2b8fe7f238",
      "fbf46348797c4627b88650f33be15f77",
      "c1f70f53890c41cab83f89d4183ed8f3",
      "2b0726f00e97418d891e2dd0a59c2c5a",
      "ac713967f51d4f1fa6e02f076cff227f",
      "193e3dca426f442bbd2e2c8f0c491e4f",
      "d6bab5e81bef44b2a764a3ff5f50817d",
      "d5c30c247d1f477fb18247a1a75316cc",
      "2b066f1930004fe59782bed9eb12b8a6",
      "f4e1b81472ec4c7ab49c542e64953bb5",
      "a80f0db6eff14457bd09a5972d5c64f9",
      "2a197b18faf44319bdefff7dc4e4dff8",
      "fd00ee28ce8f43f29ea89e23c4ceb6f3",
      "fd09bc3eb0404f559340958d78163dd9",
      "f0b528288f4140cdb9ef120279fb18cd",
      "78afaf345aa34a0585de3b20a6908b11",
      "731513f4a3f0403e881aac87f05fe269",
      "87fbb240446446f29a66703e9a685022",
      "daa87c9204ec4848a501bd432039b858",
      "f639e1a66fcd41559879fc6739bf3dfc",
      "ac25c6a93b3c42cf87c0bc01c198f438",
      "2b3db1918e284a1aa88892ccbb762b2e",
      "f8ab0595b5c64f0b9e6a0690f5a72902",
      "c3b3d48d41fc4bb98e7a38fa4dbb4218",
      "5d82a9393327412b96b41f05ecdfbeb5",
      "7c4ede2c4be34a45a0a8593b54c013f0",
      "d88fbaaef13243fba9c8a654a582c212",
      "7041aa71a7c54b32a6a70f5908554fad",
      "a96e38f318df404291aaffa1bd126b09",
      "e4baca2508bb4429a0ee8b4d8a6b6050"
     ]
    },
    "id": "s_AY1ATSIrIq",
    "outputId": "74197b0a-8c01-42e9-85eb-89c0ea2675df"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", actual_task, trust_remote_code=True)\n",
    "metric = load_metric('glue', actual_task, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual record, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5GqPC4D_L3Z",
    "outputId": "92cfccf5-c48d-44b2-b2eb-08bcce230a63"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1Pkgdai0kwN"
   },
   "source": [
    "Let's look at one record in the train split they provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6HrpprwIrIz",
    "outputId": "dd3ce02e-41f6-49d3-af09-99542693e47c"
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS9Xwkf-7ZdY"
   },
   "source": [
    "**QUESTION:**\n",
    "\n",
    "1.a. How many records in the train split of the sst dataset we're using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kC6uSXnv7ZdY",
    "outputId": "20a8a2b0-4ec4-473e-ff86-c977d4fcebe0"
   },
   "outputs": [],
   "source": [
    "### Q1-a Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "### END YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEhaXKkm7djs"
   },
   "source": [
    "**QUESTION:**\n",
    "\n",
    "1.b. How many records are in the test split of the sst dataset we're using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9w1uEw567djt",
    "outputId": "19224fee-1b74-4253-b9a4-5c142214e248"
   },
   "outputs": [],
   "source": [
    "### Q1-b Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "### END YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "i3j8APAoIrI3",
    "outputId": "e6746b05-a1eb-4bb3-f898-fae71655e75c"
   },
   "outputs": [],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAWdqcUBIrJC"
   },
   "source": [
    "You can call the `compute` method associated with the glue benchmark, pass your predictions and labels directly and it will return a dictionary with the metric(s) value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XN1Rq0aIrJC",
    "outputId": "9cf1ddee-2a7f-48d0-d8fe-8bcc54a9640b"
   },
   "outputs": [],
   "source": [
    "fake_preds = np.random.randint(0, 2, size=(64,))\n",
    "fake_labels = np.random.randint(0, 2, size=(64,))\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkTwP_FN5Rmj"
   },
   "outputs": [],
   "source": [
    "sentence1_key, sentence2_key = (\"sentence\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "We need to take the text input and run it through the tokenizer to get input_ids, following (https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb).  We construct a properly formatted input for the Trainer class using the pre-process function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "2e2a7296830644129de6fec42ebc96eb",
      "4fca67e2b17744d7a5a2ca4a494c5b65",
      "3433da42323a4a5585cb5adaf79a82b4",
      "e95f14cc09ee49f0bc39b213ba65cb47",
      "63956ed08f55473b932d12169fbafb57",
      "2a5881ea96d84aa58d2387123b9d1664",
      "94433fe6192b43b9abdd46be10eea088",
      "e73a1e76b99a4e259f0702fdbf7e3565",
      "0903e3bbb51d4244ab12cdbbcc435890",
      "0df0db85723a40a6b576c88b1593edee",
      "e18bd5521094493cbc7a247ba4b4edc8",
      "fe873604c50f462c9dfce2c880494452",
      "6d9c843b979f439d8c9e66dbdfe6c648",
      "6612a8559c694052909f53a82da8bfa3",
      "f733525d16964cbdbc509e5886266eb2",
      "31ec6704ec154374a60bf9d4815d54b3",
      "32726028dc9d47ddbcaecc4d7651c9ac",
      "ec752fd18830486d9986a22f2993c57e",
      "2f27d45f9adc475b90f1733891ec2718",
      "d2881ca6498f41b8ad91d20d26491ac5",
      "5af7d4a257244459825014a0eb99540d",
      "7856fc8fe94e495aa15d782ee14f29a3",
      "4d9224c418de475f8d61ca4442500d80",
      "9af810f4d81946a7ba43d79b683c120e",
      "5d2069e2f624496ab18f02c068a37e13",
      "f3ca240fd46141548c86dce8f275ff74",
      "07d2b7d4a05542efa17de7c4370efb39",
      "0187bfeacb844d149c83394485e473e8",
      "51e6144731664b358baff13c27808c44",
      "f133fe3fd2d6455089befef5a7e10dfc",
      "7869207d99214bc88fdce1c6fb51292f",
      "f713ac8fa6b44f109ffacfccc3bb425e",
      "70d006453a4a4ffa8d617e53e884cd8d"
     ]
    },
    "id": "DDtsaJeVIrJT",
    "outputId": "2f263547-5289-47b6-8b7a-61f15f117ae8"
   },
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaPKCdr2sQJp"
   },
   "source": [
    "Lastly, we define for the future analysis the base model, the metric, and the key for the validation data in the encoded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rF0LK6xPV85P"
   },
   "outputs": [],
   "source": [
    "metric_name = \"accuracy\"\n",
    "\n",
    "validation_key = \"validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4poBhZZIEMy7"
   },
   "source": [
    "##2. QLoRA Setup\n",
    "\n",
    "Now let's use QLoRA to fine-tune a model that is quantized down to a much smaller bit representation. We first need to specify the BitsAndBytes configuration, then the LoRA adapter, and then we'll train as always. But now we will use the [large model](https://huggingface.co/openai-community/gpt2-large) with 812 million parameters. That would **not** fit into our T4 chip for training purposes. It will work with QLoRA! How good will the results be?  Let find out.\n",
    "\n",
    "First, your need to fill in the values of the BitsAndBytesConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDmx1SUaTdjT"
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "    ### END YOUR CODE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DVIY9Ib-GV7"
   },
   "source": [
    "**QUESTION:**\n",
    "\n",
    "1.c. What is our quantization goal - 16 bit, 8 bit or 4 bit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNmPIm96-GV8",
    "outputId": "71f8b901-1f8e-420e-a1b4-0df7d7513d6d"
   },
   "outputs": [],
   "source": [
    "### Q1-c Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "### END YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sc4Dz38t2RMw"
   },
   "source": [
    "Let's take advantage of Hugging Face's AutoModel classes.  We're doing classification so we're going to want the AutoModelForSequenceClassification class. They've already attached an output layer for us so we simply need to load the model weights and we can fine tune our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "3c4a8355c5ab4d3ea98626f14fd0dc19",
      "66c3dbbf3e1344028f5c756c8db284d4",
      "e118dc5a1dfc480592738b321d9d1827",
      "77db38b8bc1e4f37a72927b8858b7159",
      "dfe904d04f0a45baa6c31461dc0cf7ad",
      "291d1156fd3648daa252031e6aee34ba",
      "bd5b57cc313148288375b55e59bebaa8",
      "852e11fc3bcd4a17beb0aa57270eef2b",
      "13fd8f2d040543d8b90e6b73cca6cfe1",
      "b82019f1b62049a5a4908631455ed7a2",
      "7b8a5db28ba547f0ab0865f1d2b99e39"
     ]
    },
    "id": "a8Y6wLYpTdmb",
    "outputId": "baa0fdb3-f15e-4811-8005-f5c09628f58d"
   },
   "outputs": [],
   "source": [
    "qlora_model = AutoModelForSequenceClassification.from_pretrained(base_model_name, quantization_config=bnb_config, device_map={\"\":0})\n",
    "\n",
    "qlora_model.config.pad_token_id = qlora_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgfyKAqV25e1"
   },
   "source": [
    "We can see the components of the model below. It tells us how the underlying decoder is structured.  You can see linear layer they've attached.  I generates two outputs -- one will be for positive e.g. 1 in the label and a second for negative e.g. 0 in the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZwWXf1DjOdqf",
    "outputId": "8dc256ce-38ef-4e27-e40c-1ab97633f0c7"
   },
   "outputs": [],
   "source": [
    "print(qlora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtPa3Iu4-36y"
   },
   "source": [
    "**QUESTION:**\n",
    "\n",
    "1.d. What kind of activation function is used in the MLP portion of the model?  RELU, LRELU, SPELU, or GELU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "E-ZHowa1-360",
    "outputId": "de042da7-b5b9-4153-e4a2-45a6a94d2ae8"
   },
   "outputs": [],
   "source": [
    "### Q1-d Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "### END YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKaHrx963YgG"
   },
   "source": [
    "Looking at the contents of model.config can also be very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBGyArCQOI4G",
    "outputId": "233d0c1e-1634-4275-f9fb-ce2242d9c0cf"
   },
   "outputs": [],
   "source": [
    "print(qlora_model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQM8NpPV9O_C"
   },
   "source": [
    "Now let's see how much of the GPU memory is consumed by the model BEFORE we add in the adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgvXAb3sVwq2",
    "outputId": "81b3a393-d673-44f7-a8a2-8572a666d8f0"
   },
   "outputs": [],
   "source": [
    "show_currently_allocated_gpu_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo9f5L-mEqvk"
   },
   "source": [
    "We need to do a few more adjustments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmRFl_AIUY2u"
   },
   "outputs": [],
   "source": [
    "qlora_model.gradient_checkpointing_enable()\n",
    "qlora_model = prepare_model_for_kbit_training(qlora_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VVPg7ac9pLW"
   },
   "source": [
    "Now, very importantly, we need to set the size of our adapter.  Specifically we need to set the value of the rank and the alpha.  You can read about the purpose of these values in [this excellent blog](https://www.determined.ai/blog/lora-parameters) about fine-tuning with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zhb5E5hCUY7K",
    "outputId": "cac4d779-85d2-4863-80c2-b856f43c3356"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "qlora_model = get_peft_model(qlora_model, config)\n",
    "qlora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ytRWVG1MpoJ",
    "outputId": "7790cf25-1946-4c1b-f430-cceda3676f13"
   },
   "outputs": [],
   "source": [
    "qlora_model(**tokenizer('this is fun', return_tensors='pt').to('cuda'))['logits']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KlRLbCf-4pi"
   },
   "source": [
    "Now, let's see how much memory the LoRA adapter is using.  By looking at the difference between the GPU memory allocation before and after we can see how much space the adapter is using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Z_UCrx9OmIN",
    "outputId": "164bd2c6-72e7-45a3-fd4f-06d7eee25754"
   },
   "outputs": [],
   "source": [
    "show_currently_allocated_gpu_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puOt3vUy_aQW"
   },
   "source": [
    "Now we need to configure the Hugging Face Trainer.  This involves filling out the Training Arguments structure so we can pass it on the to Trainer class when we instntiate it.  As indicated in the blog you need to find a good learning rate that will allow you to get an evaluation accuracy above 0.92.  You can experiment with multiple values.  Once you find a good one you can shift to finding good r and alpha values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PF6g9YXUX350",
    "outputId": "05147135-fe31-4d24-f639-2082c71a4620"
   },
   "outputs": [],
   "source": [
    "## Training Arguments structure\n",
    "args = TrainingArguments(\n",
    "    f\"qlora_{base_model_name}-finetuned-{task}\",\n",
    "    eval_strategy = \"steps\",\n",
    "    eval_steps = 100,\n",
    "    save_strategy = \"no\",\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = 100,\n",
    "    learning_rate=1,                   ### YOUR EXPERIMENT VALUE HERE!\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=300,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=metric_name,\n",
    ")\n",
    "\n",
    "\n",
    "## Trainer we'll use to fine tune\n",
    "qlora_trainer = Trainer(\n",
    "    qlora_model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRFnY9ML577Z"
   },
   "source": [
    "Now that we have configured the trainer we can easily train the model by simply calling trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "ftTHATekX38l",
    "outputId": "15828e0b-c785-4542-d5c7-a505c9525fc9"
   },
   "outputs": [],
   "source": [
    "qlora_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAGpciEd6SXI"
   },
   "source": [
    "Let's evaluate the trainer against our validation test set and see how well our model is performing.  The trainer class simplifies the evaluation process as well.  Your goal is to get an 'eval_accuracy' above 0.92 by manipulating the values for r, lora alpha, and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "-WKJMD1-r2E0",
    "outputId": "5b9fe6f7-0603-4b2e-b23b-96cabddd0681"
   },
   "outputs": [],
   "source": [
    "qlora_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbUldNZ65Kbj"
   },
   "source": [
    "**QUESTION:**\n",
    "\n",
    "1.e. What is the value of the learning rate that allows you to get an evaluation accuracy above 0.92?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "XZtOuWn4EhZY",
    "outputId": "798460a6-f24f-4dad-9933-47c8d525a780"
   },
   "outputs": [],
   "source": [
    "### Q1-e Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "### END YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Daz6twyaBQUX"
   },
   "source": [
    "**QUESTION:**\n",
    "\n",
    "1.f. What is the r value of your LoRA adapter that lets you get an evaluation accuracy above 0.92?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBXzu2dBBQUX",
    "outputId": "f5c53296-396a-4b96-88e4-1aea5f2f852f"
   },
   "outputs": [],
   "source": [
    "### Q1-f Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "### END YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouLhzZ2OBRC3"
   },
   "source": [
    "**QUESTION:**\n",
    "\n",
    "1.g. What is the r-alpha value of your LoRA adapter that lets you get an evaluation accuracy above 0.92?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3pwtMiTBRC4",
    "outputId": "6624d302-40a3-4134-b390-240dc78c05aa"
   },
   "outputs": [],
   "source": [
    "### Q1-g Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "### END YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVvN7sTwGoN8"
   },
   "source": [
    "Be very careful using this as it will clean out memory.  Only use it once you've identified the values you're happy with in fine-tuning GPT2 and you're ready to move on to the next model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYrDR14LRjtB"
   },
   "outputs": [],
   "source": [
    "del qlora_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eID9zRT5EiFg"
   },
   "source": [
    "## 3. Larger Model Data Setup\n",
    "\n",
    "Now let's try running the same data through a more recent model, specifically Gemma2 from Google.  You can check out [the model card](https://huggingface.co/google/gemma-2-2b) -- always a good idea or the [Technical Report](https://arxiv.org/pdf/2408.00118). We again need to define the tokenizer for our second model so we can reprocess the data.\n",
    "\n",
    "In order to use this model you will need to identify yourself to HuggingFace. If you don't already have an account with Hugging Face, you should get one.  You can then log in to the Hugging Face site and from there go to [the model card](https://huggingface.co/google/gemma-2-2b) and request access to the Gemma model.  Once you are granted access -- usually a matter of minutes -- you will see something like the following when you visit the model card:\n",
    "\n",
    "![Screenshot 2025-02-13 at 15.30.42.gif](data:image/gif;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9EQf34kEiFh"
   },
   "outputs": [],
   "source": [
    "task = actual_task = \"sst2\"\n",
    "large_model_name = \"google/gemma-2-2b\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "ed8af75a5e424c80b2c3d0384f7d58c3",
      "43e97c5b574d4e9e8724c7384228dc7d",
      "cfe0d4fcc3cb46ed8038bc6062be09f4",
      "ea21326e65824f44b8de6910e5b15cf0",
      "4a9ed910bf4444b5a89bfed7de4c5dab",
      "b467b8fb38ec46e38a8cd011521bd18d",
      "e14fbcdf2e304027bc6b93d1fba41667",
      "c4d360e2256e461fae9d27d2357aa6e9",
      "cfff9fd25807468e951928be8b530f4f",
      "72030cd32d994f198c0658eac260123e",
      "68077a5ad22b4028971196dcbd565b39",
      "0e79a12e7e9e4e90b4982cc4e2c62094",
      "7d4b13cad2244b149b1449a58b2693fb",
      "437192fceb84473588113abc902da63e",
      "3b05ec33897741d7afc670e0e9d22683",
      "397889d3f0f54da4945c85c5cbd05ad6",
      "e86d607676064385ae2a81d8714e07cb",
      "e5e8b3f3a04d4dd3bd77cd91d02e3106",
      "a2b6ea7be06247d29fcf4a7874bf116b",
      "524e7d92aafe434d8fa5c0b4a3b4229b",
      "f5198e64d3f34ff184d3bdfc043cd312",
      "c6c9811e3ba14bb394a3ac7c07a7ffff",
      "095879491bf1480ab64e482e0c37f82b",
      "4c595731fb2f45d694d88f9488c65a24",
      "3cdafaa04c534604be5e5ab41081b86a",
      "a99dee2385214e63841f876a548a0bef",
      "ddba8054a454450b876e690c8c793816",
      "33dbd7a29c374e90ada8450bace8c7d8",
      "fa717fcbf01248ec96d1361b0daee9df",
      "206d42b10cb9459fac669cac4f31709e",
      "54704fae9e2b44ec8775947fbd50d5b5",
      "882ba62bc0b04010a5aaf631a36c646f",
      "69d1d12e387f4a30bee32cd096cfb598",
      "bc4a9592d1654a72b9d2e20f8f414d18",
      "716ea3e71c48477ea1c98c8256ca9551",
      "24efecedf1104084a1abb3999a114863",
      "8ab1c3a7414e4d8c9c19366566a00830",
      "a5ec2e1079aa4bce912bc62af919854a",
      "dd5f87f186334634a96edefc76b5a6ee",
      "83d1408bd7924e24b311e7ddafbaeb37",
      "e608606d4b0e482aa2ce5f42c1266e13",
      "3e07332fff8c4536af43cf8f02b45ade",
      "6b0e3d2c2afd4ce48d4bf52f7d968a46",
      "b59f94840c414e1f8210e1491b951b1b"
     ]
    },
    "id": "Qo1fp1mtEiFh",
    "outputId": "0472a848-1ab3-4501-f4f2-99c21d1cfb7a"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(large_model_name, use_fast=True,)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XymN5atjEiFi"
   },
   "source": [
    "We're using a new model and it has its own tokenizer so we need to reprocess the data with that new tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130,
     "referenced_widgets": [
      "17171172f89c43cdb943f83155420e80",
      "56ace9573d064a02b7ce0f886eb8881f",
      "bb0ad342dd624df0a96e5ff344313f34",
      "d5fce535baff46958c98c01afc6d0de8",
      "5b712e9c18284a64ba5abd6f4ab28cda",
      "67a09150268f4de59593c57695e56390",
      "fa8984d01071441eb8bbecbef3d12307",
      "e650f4deda56446fb5f29f10bb9a5b72",
      "0eee58f13a5f4806aac97839aac2560a",
      "192df4e1364d4ddeaf67d11abfd86761",
      "5f40dc6ccab04e9498ba35059b6e51e3",
      "0b7aa9835c844084bf00a94167ca5ec5",
      "1caa052f8a974ab7a514476845cd6383",
      "cdbc65601ad74bed96a4d01472575f74",
      "fd72e3421d8f4212ac15a59f95dbc623",
      "5f80aa62005e442bb6c337a463e50e03",
      "17a428e4b2b34d1e81d91a7366905f2b",
      "2690ab599ce045408032ea23f0a0baab",
      "59f3883b88bf4a0584ec51501c6bc4a7",
      "e763a727be8845069a8de7c65296544a",
      "b690a931b0e74d4ebb55b02d44dd23d7",
      "519e49e14f06442d82b1038d25e8abd6",
      "aa99dcfdd3bf4d78b37cef68840ee93c",
      "995faadd5e754e6fb277d9a52d1652ba",
      "e882d4daa6cd4716b58ac9f7d707f5a4",
      "726b33e3ce624af898cb26440ac88265",
      "5f7586148ffb474b83753eb562f5548e",
      "53c5b7f6d7394ddb8971ddd7aae7d84f",
      "e20fbc35be734d0e8f00c7b3cdaf0402",
      "88ae449385994c2cbce9a884e6003544",
      "df884c246a5d4db5a9af7b9e1576ffc6",
      "ea237b858ca9487689570072544bbd08",
      "b5bbad319a5c421cb87d602f47aedb85"
     ]
    },
    "id": "dlbmKEo0EiFi",
    "outputId": "44f682d9-b2dc-42a9-9144-08edc7ddb189"
   },
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmYXrj5iEiFi"
   },
   "source": [
    "Lastly, we define for the future analysis the metric and the key for the validation data in the encoded dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9reHFregEiFi"
   },
   "outputs": [],
   "source": [
    "metric_name = \"accuracy\"\n",
    "\n",
    "validation_key = \"validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seqLS3m-EiFi"
   },
   "source": [
    "##4. Larger QLoRA Setup\n",
    "\n",
    "Now let's use QLoRA to fine-tune a model that is quantized down to a much smaller number of bits. We first need to specify the BitsAndBytes configuration, then the LoRA adapter, and then we'll train. But now we will use a very recent LLM [and larger model](https://huggingface.co/google/gemma-2-2b) with 2.61 billion parameters. That would **not** fit into our T4 chip for training purposes. It will work with QLoRA. And how good it be relative to the GPT2 model we fine-tuned first?\n",
    "\n",
    "First you need to fill in the BitsAndBytesConfig file again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDBz4PZNEiFj"
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "    ### END YOUR CODE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeOCe6HvEiFj"
   },
   "source": [
    "Let's take advantage of Hugging Face's AutoModel classes.  We're doing classification so again we'll use the AutoModelForSequenceClassification. They've already attached an output layer for us so we simply need to load the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296,
     "referenced_widgets": [
      "f3736b69f4604f59a14f0e3794c396cb",
      "5bcd99fb56884425bd8228bc4b45ddf1",
      "bb3b80aa49e34e7d8619efd943b8ec33",
      "8046e425cfba4e5ea9a418b9d3739fe9",
      "dc6152d154f04ddbb352789fc4a39066",
      "8e1664a7e4de4d4ab9a789513831ab23",
      "535567691ad34156827dcefbcb9e6fab",
      "50d720cda02b41739a3489a5ef6fdb62",
      "04a32069fa7e452b9c5b2403999c6d20",
      "2d6f44a3bb204f8fac6401b5eca9535c",
      "d3b9b57e0df14b40b48581b721b714eb",
      "c829661dc0b74e05a081f43a8dc34140",
      "b4a2dec0eb084b8b8ad4b783b1e58b76",
      "0f621fa2761e42809ce9bb05909a11f6",
      "f56495bf874643c6810ff0d6f20ad5fd",
      "c2834288e5e04c70a4f875463ea041b8",
      "0249ee77d24d4d0b92df69983ef05bea",
      "6034e196fb5c4a3b84949f3e907bed2e",
      "c1a4a9d951c14cbdafa5300d15f4f3ad",
      "9259d12f8e27442d89d49946efccad70",
      "71bf29c4eafc4ef9a461c142da722ce2",
      "00ae9759522142bdb2d4d96f608aeaf9",
      "2773d2d18ff74dffbf420e781997f647",
      "3e631de2b3b8461581523a771a0f3391",
      "56e3406149d54d33b620e1b09593ef9b",
      "9b87d3f30fe142268cc8a77f705c73cc",
      "26ea8927aab841aa82541eaaf6d49432",
      "56b6afdaac51400fb1dd69fe9b795327",
      "e318bc31a24445b2826163d14fbdcbb7",
      "7b77154a6aa449fb8208b461fef149f5",
      "bf34ee403260440b89f7048138ded1e6",
      "e9b3feceb95b417fa3191188f0f5f602",
      "edacd8021cb148fab01ca41a2f8ca73d",
      "9f717a00d33f42f68d7d626e6bb395b2",
      "679f666d874e4bd7924070ac16e62d23",
      "6159fbc9b9c04332b4e542730f723df3",
      "906ee84da54e44d9844c4d9ca96e78d0",
      "866383b675344cd4aa778a2ba17c2b2c",
      "3c3bd30bcc3142e3bbcae0c1787a0df6",
      "e3525ee1597545e7993b46218ff015cd",
      "9a37266b705748879af9b64bdb75993c",
      "192c0c9dd4d044c8a133c1beada89f47",
      "2753082cf9ec4396a776344340bf36b9",
      "14569335fa4a40ac85920e68cf9f6026",
      "82408911f2e34db5899f64509c633f13",
      "1a444725790244698376aeb8f60872f8",
      "dede324b88744e3a81c00e600a4b5516",
      "dc608be72e09419880ebb8e5f22f7c58",
      "1f14bdcefcd444d491c8a62732425812",
      "52d741043d664f42b7257d2642c9ae2a",
      "24bb07e5b61a48c18b40d1a0f6f66ec7",
      "ccf8e29578fe47818b897adc10eba106",
      "3c347f67551a42a49cfdd354f931e9e2",
      "44a6d0398d83404082a99529d75c5270",
      "1d1cb24976f745ec8fcafd0b24be18d0",
      "3e943a775c4e42a083bf6af4a50b3020",
      "23cfef7cd87341f192311c79876951dd",
      "03e0046deeef433a9c93949d6d86a7ca",
      "2ba446c63fed4de1bf221500c04600f6",
      "1e487dd787ce47fe94f7d25735418ef0",
      "c5fd3e40666a4ff186cb135abf18b19c",
      "c4272573fd4346b185f8fc8265a29484",
      "67e50c926fcf4df4887df0787e09b8af",
      "dae66b8a135d45cbab01eb3dbfeeb902",
      "64c8d298fe09436caf0bf80ca6c4941f",
      "967223fa31a246c4baa1362c63623a89",
      "fff0813f966b4e9d9087bc5187723fb0",
      "68367ba3e9854a18ad4758b740ed9a51",
      "ab94be97e8074667847c3097236a34d6",
      "ab0652574ab348faaddbc09de817f506",
      "b2d7354479a446608dd808d4929b0a06",
      "d0a4f6fa37cf47349243548fe0921da9",
      "85e3752ebfb24d879d08375d7ac172b7",
      "f00d24293ce34593ae5d7bf792a27b67",
      "88a1aba006834d598ff6a48e84d247b9",
      "3dd0fbe9133e4850877fd1c9db33d4c7",
      "87d0d46ac57546759522628a94534e3c"
     ]
    },
    "id": "-4U2i7iNEiFj",
    "outputId": "4f5e2a8f-6eeb-4e42-abd9-0392c476240f"
   },
   "outputs": [],
   "source": [
    "lqlora_model = AutoModelForSequenceClassification.from_pretrained(large_model_name, quantization_config=bnb_config, device_map={\"\":0})\n",
    "\n",
    "lqlora_model.config.pad_token_id = lqlora_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1454vg7FEiFj"
   },
   "source": [
    "We can see the components of the model below. It tells us how the underlying decoder is structured.  You can see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9IgF8B5EiFj",
    "outputId": "ca6ca7d9-76a9-4077-cb36-9afc84f3bd52"
   },
   "outputs": [],
   "source": [
    "print(lqlora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fi_gc4FFEiFj"
   },
   "source": [
    "Looking t the contents of model.config can also be very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ypjhtj_0EiFj",
    "outputId": "5ceff48a-f17b-4da0-a232-15274853a802"
   },
   "outputs": [],
   "source": [
    "print(lqlora_model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpD0CKS7Np67"
   },
   "source": [
    "Now that we've loaded the Gemma model into memory let's see what size footprint it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFv_aA2REiFj",
    "outputId": "c9026b90-b5be-4827-95e5-da5ae3b44dad"
   },
   "outputs": [],
   "source": [
    "show_currently_allocated_gpu_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAROPH0yEiFj"
   },
   "source": [
    "We need to do a few more adjustments to take advantge of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugkDwNnbEiFj"
   },
   "outputs": [],
   "source": [
    "lqlora_model.gradient_checkpointing_enable()\n",
    "lqlora_model = prepare_model_for_kbit_training(lqlora_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRq9OGIaGA3M"
   },
   "source": [
    "Now you can experiment a bit with the values for r, lora_alpha, and the learning rate.  For staters try using the values you landed upon when you were fine-tuning GPT2-large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0MLYGyYcEiFj",
    "outputId": "e9456470-33d0-42e4-c6b9-5a8b76f4593f"
   },
   "outputs": [],
   "source": [
    "lconfig = LoraConfig(\n",
    "    r=0,             ###### YOUR VALUE HERE\n",
    "    lora_alpha=0,    ###### YOUR VALUE HERE\n",
    "    lora_dropout=0,  ###### YOUR VALUE HERE\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "lqlora_model = get_peft_model(lqlora_model, lconfig)\n",
    "lqlora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNkJ4CFyCysH"
   },
   "source": [
    "**QUESTION:**\n",
    "\n",
    "1.h. What is the r value of your LoRA adapter that lets you get an evaluation accuracy above 0.95?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXUDVhFrCysI"
   },
   "outputs": [],
   "source": [
    "### Q1-h Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "### END YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZV1ngN-CzGV"
   },
   "source": [
    "**QUESTION:**\n",
    "\n",
    "1.i. What is the r-alpha value of your LoRA adapter that lets you get an evaluation accuracy above 0.95?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BS_ccCqdCzGV"
   },
   "outputs": [],
   "source": [
    "### Q1-i Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "### END YOUR ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUEbmVpdEiFj",
    "outputId": "ed147ca5-93e5-43db-df94-8431f7e4a6bc"
   },
   "outputs": [],
   "source": [
    "#qlora_model.to('cuda')\n",
    "lqlora_model(**tokenizer('this is fun', return_tensors='pt').to('cuda'))['logits']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiYQftEiCPf6"
   },
   "source": [
    "**QUESTION:**\n",
    "\n",
    "1.j. What is the number of trainable parameters in the QLoRA model for Gemma-2-2B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Eki9LufzCPf7",
    "outputId": "79d6a5ab-a9b6-4dfb-a437-a784ca18c863"
   },
   "outputs": [],
   "source": [
    "### Q1-j Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "### END YOUR ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "llW_napkEiFk",
    "outputId": "b9c919ce-3bbf-480e-b103-7889786f1e4e"
   },
   "outputs": [],
   "source": [
    "show_currently_allocated_gpu_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uar0n0m6EiFk",
    "outputId": "8e8ee45f-2f4d-40ae-d38f-711f11ce4197"
   },
   "outputs": [],
   "source": [
    "largs = TrainingArguments(\n",
    "    f\"lqlora_{large_model_name}-finetuned-{task}\",\n",
    "    eval_strategy = \"steps\",\n",
    "    eval_steps = 100,\n",
    "    save_strategy = \"no\",\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = 100,\n",
    "    learning_rate=0,                ####### YOUR VALUE HERE\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=300,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")\n",
    "\n",
    "lqlora_trainer = Trainer(\n",
    "    lqlora_model,\n",
    "    largs,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6x3Zz_ZC8s7"
   },
   "source": [
    "**QUESTION:**\n",
    "\n",
    "1.k. What is the learning rate you are using for the larger QLoRA model to get a validation accuracy above 0.95?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onb43GzlC8s8"
   },
   "outputs": [],
   "source": [
    "### Q1-k Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "### END YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjvGdBOMEiFk"
   },
   "source": [
    "Now that we have configured the trainer we can easily train the model by simply calling trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "iM9PP6DlEiFk",
    "outputId": "8c81c79f-39c6-4185-fa2d-ff576e76ad53"
   },
   "outputs": [],
   "source": [
    "lqlora_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1m34AWR-EiFk"
   },
   "source": [
    "Let's evaluate the trainer against our validation test set and see how well our model is performing.  The trainer class simplifies the evaluation process as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "zORSJCfNEiFk",
    "outputId": "d8360386-e5ae-40bf-bced-caaeec0a0290"
   },
   "outputs": [],
   "source": [
    "lqlora_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IIG7pVZDsER"
   },
   "source": [
    "**QUESTION:**\n",
    "\n",
    "1.l. What is the final evaluation accuracy you get on the larger model?\n",
    " (Must be above 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4RoQchBDsES"
   },
   "outputs": [],
   "source": [
    "### Q1-l Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "### END YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5h4L-lAO_Ti"
   },
   "source": [
    "Okay, youre done with assignment III.  Hopefully you've had a gentle introduction to using quantization and LoRA to fine tune different decoder based models."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
